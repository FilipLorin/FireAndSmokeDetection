{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6d10277-d8b0-4980-81ca-1d6892e86a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D-Fire downloaded\n",
      "Downloading FASDD_CV\n",
      "Dataset URL: https://www.kaggle.com/datasets/cookiecacheqq/fasdd-cv\n",
      "FASDD_CV Downloaded\n",
      "FASDD_CV extracted\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Roboflow datasets downloaded\n"
     ]
    }
   ],
   "source": [
    "# Dowload datasets\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() # API Keys are stored in .env file\n",
    "\n",
    "os.makedirs(\"datasets\", exist_ok=True)\n",
    "\n",
    "#======================================\n",
    "# Load D-Fire dataset from Google Drive\n",
    "#!pip install gdown\n",
    "import gdown \n",
    "from zipfile import ZipFile\n",
    "df_url = \"https://drive.google.com/uc?id=19LSrZHYQqJSdKgH8Mtlgg7-i-L3eRhbh\"  # smoke = 0, fire = 1\n",
    "df_destination = \"datasets\\\\D-Fire.zip\"\n",
    "df_directory = \"datasets\\\\D-Fire\"\n",
    "\n",
    "if not os.path.isdir(df_directory):\n",
    "    if not os.path.isfile(df_destination):\n",
    "        print(\"Downloading dataset...\", end='\\r')\n",
    "        gdown.download(df_url, df_destination, quiet=False)\n",
    "        print(\"D-Fire dataset downloaded\")\n",
    "    else:\n",
    "        with ZipFile(df_destination, 'r') as zip_ref:\n",
    "            print(\"Unpacking...\" , end=\"\\r\")\n",
    "            zip_ref.extractall(df_directory)\n",
    "            print(\"Files extracted\")\n",
    "print(\"D-Fire downloaded\")\n",
    "\n",
    "#======================================\n",
    "# Load FASDD Dataset from Kaggle\n",
    "#!pip install kaggle\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "k_dataset = 'cookiecacheqq/fasdd-cv' # smoke = 1, fire = 0 !\n",
    "k_path = 'datasets'\n",
    "k_dir = 'datasets\\\\FASDD_CV'\n",
    "kapi = KaggleApi()\n",
    "\n",
    "print(\"Downloading FASDD_CV\")\n",
    "if not os.path.isdir(\"datasets\\\\FASDD_CV\\\\FASDD_CV\"):\n",
    "    kapi.dataset_download_files(k_dataset, k_path)\n",
    "    print(\"Extracting...\")\n",
    "    with ZipFile(k_path + \"\\\\fasdd-cv.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(k_dir)\n",
    "print(\"FASDD_CV extracted\")\n",
    "\n",
    "# Convert to YOLO Format and swap labels\n",
    "import re\n",
    "\n",
    "def swap_labels(data):\n",
    "    d = re.sub(\"0 \", \"2 \", data)\n",
    "    d = re.sub(\"1 \", \"0 \", d)\n",
    "    d = re.sub(\"2 \", \"1 \", d)\n",
    "    return d\n",
    "\n",
    "r = \"datasets\\\\FASDD_CV_Converted\"\n",
    "#   1 - ensure filetree exists\n",
    "os.makedirs(r, exist_ok=True)\n",
    "for d in [\"train\", \"test\", \"valid\"]:\n",
    "    os.makedirs(os.path.join(r, d), exist_ok=True)\n",
    "    for dd in [\"images\",  \"labels\"]:\n",
    "        os.makedirs(os.path.join(r, d, dd), exist_ok=True)\n",
    "\n",
    "src = \"datasets\\\\FASDD_CV\\\\FASDD_CV\\\\FASDD_CV\\\\annotations\\\\YOLO_CV\"\n",
    "img_src = \"datasets\\\\FASDD_CV\\\\FASDD_CV\\\\FASDD_CV\\\\images\"\n",
    "for f, d in zip([\"test.txt\", \"train.txt\", \"val.txt\"], [\"test\", \"train\", \"valid\"]):\n",
    "    with open(os.path.join(src, f), 'r') as llist:\n",
    "        labels = llist.readlines()\n",
    "    for l in labels:\n",
    "        img_name = l.split(\"/\")[-1].strip()\n",
    "        l_name = re.sub(\".jpg\", \".txt\", img_name)\n",
    "        #  2 - copy label files and swap indexes\n",
    "        with open(os.path.join(src, \"labels\", l_name), 'r') as lab:\n",
    "            data = lab.read()\n",
    "        with open(os.path.join(r, d, \"labels\", l_name), 'w') as lab:\n",
    "            lab.write(swap_labels(data))\n",
    "        # 3 - link img files\n",
    "        os.symlink(os.path.join(img_src, d, \"images\", img_name), os.path.join(r, d, \"images\", img_name))\n",
    "\n",
    "#======================================\n",
    "# Load datasets from Roboflow\n",
    "#!pip install roboflow\n",
    "from roboflow import Roboflow\n",
    "\n",
    "roboflow_api_key = os.getenv(\"RF_API_KEY\")\n",
    "rf = Roboflow(api_key=roboflow_api_key)\n",
    "\n",
    "# Download roboflow datasets\n",
    "os.chdir(\"datasets\")\n",
    "rf_fas = rf.workspace(\"data2\").project(\"fire-smoke-detection-ua3dm\").version(2).download(\"yolov11\") # fire = 0, smoke = 1 !\n",
    "rf_wfs = rf.workspace(\"brad-dwyer\").project(\"wildfire-smoke\").version(1).download(\"yolov11\") # smoke = 0\n",
    "print(\"Roboflow datasets downloaded\")\n",
    "os.chdir(\"..\")\n",
    "\n",
    "# Swap labels for rf_fas \n",
    "\n",
    "r = \"datasets\\\\fire-smoke-detection-2_Converted\"\n",
    "#   1 - ensure filetree exists\n",
    "os.makedirs(r, exist_ok=True)\n",
    "for d in [\"train\", \"test\", \"valid\"]:\n",
    "    os.makedirs(os.path.join(r, d), exist_ok=True)\n",
    "    for dd in [\"images\",  \"labels\"]:\n",
    "        os.makedirs(os.path.join(r, d, dd), exist_ok=True)\n",
    "\n",
    "src = \"datasets\\\\fire-smoke-detection-2\"\n",
    "for d in [\"test\", \"train\", \"valid\"]:\n",
    "    for l_name in os.listdir(os.path.join(src, d, \"labels\"):\n",
    "        img_name = re.sub(\".txt\", \".jpg\", l_name)\n",
    "        #  2 - copy label files and swap indexes\n",
    "        with open(os.path.join(src, d, \"labels\", l_name), 'r') as lab:\n",
    "            data = lab.read()\n",
    "        with open(os.path.join(r, d, \"labels\", l_name), 'w') as lab:\n",
    "            lab.write(swap_labels(data))\n",
    "        # 3 - link img files\n",
    "        os.symlink(os.path.join(src, d, \"images\", img_name), os.path.join(r, d, \"images\", img_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd285d94-7cb8-48aa-8bb9-8dcccce43276",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build data.yaml\n",
    "yaml_data = f\"\"\"path: {os.getcwd() + \"\\\\datasets\"}\n",
    "train: \n",
    "- D-Fire\\\\train\n",
    "- FASDD_CV_Converted\\\\train\n",
    "- fire-smoke-detection-2_Converted\\\\train\n",
    "- Wildfire-Smoke-1\\\\train\n",
    "val: \n",
    "- D-Fire\\\\test\n",
    "- FASDD_CV_Converted\\\\valid\n",
    "- fire-smoke-detection-2_Converted\\\\valid\n",
    "test: \n",
    "- fire-smoke-detection\\\\test\n",
    "- Wildfire-Smoke-1\\\\test\n",
    "\n",
    "nc: 2\n",
    "names: ['smoke', 'fire']\n",
    "\"\"\"\n",
    "\n",
    "with open(\"datasets\\\\data.yaml\", 'w') as file:\n",
    "    file.write(yaml_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33efe36f-90d4-4943-8c0a-50199c4c4acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset metrics\n",
    "\n",
    "def dataset_metrics(path): \n",
    "    # count images containing, count bboxes\n",
    "    for tld in os.listdir(path):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b5dac62-9cc6-4af7-92f9-4ad275ce36fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-629f6fbed82c07cd\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-629f6fbed82c07cd\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start Tensorflow dashboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir runs/detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263a865d-c54c-4c77-b6c1-d98243ca19a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.55 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.48  Python-3.12.7 torch-2.6.0.dev20241207+cu124 CUDA:0 (NVIDIA GeForce GTX 1050, 4096MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=datasets\\data.yaml, epochs=50, time=None, patience=100, batch=256, imgsz=224, save=True, save_period=-1, cache=False, device=None, workers=1, project=None, name=yolo_v11n_224px3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=11, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\yolo_v11n_224px3\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    431062  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,230 parameters, 2,590,214 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\yolo_v11n_224px3', view at http://localhost:6006/\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.10.cv1.conv.weight'\n",
      "Freezing layer 'model.10.cv1.bn.weight'\n",
      "Freezing layer 'model.10.cv1.bn.bias'\n",
      "Freezing layer 'model.10.cv2.conv.weight'\n",
      "Freezing layer 'model.10.cv2.bn.weight'\n",
      "Freezing layer 'model.10.cv2.bn.bias'\n",
      "Freezing layer 'model.10.m.0.attn.qkv.conv.weight'\n",
      "Freezing layer 'model.10.m.0.attn.qkv.bn.weight'\n",
      "Freezing layer 'model.10.m.0.attn.qkv.bn.bias'\n",
      "Freezing layer 'model.10.m.0.attn.proj.conv.weight'\n",
      "Freezing layer 'model.10.m.0.attn.proj.bn.weight'\n",
      "Freezing layer 'model.10.m.0.attn.proj.bn.bias'\n",
      "Freezing layer 'model.10.m.0.attn.pe.conv.weight'\n",
      "Freezing layer 'model.10.m.0.attn.pe.bn.weight'\n",
      "Freezing layer 'model.10.m.0.attn.pe.bn.bias'\n",
      "Freezing layer 'model.10.m.0.ffn.0.conv.weight'\n",
      "Freezing layer 'model.10.m.0.ffn.0.bn.weight'\n",
      "Freezing layer 'model.10.m.0.ffn.0.bn.bias'\n",
      "Freezing layer 'model.10.m.0.ffn.1.conv.weight'\n",
      "Freezing layer 'model.10.m.0.ffn.1.bn.weight'\n",
      "Freezing layer 'model.10.m.0.ffn.1.bn.bias'\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning E:\\Documents\\jupyter\\FireAndSmokeDetection\\datasets\\D-Fire\\train\\labels.cache... 17737 images, 7833 bac\u001b[0mIOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Documents\\jupyter\\FireAndSmokeDetection\\datasets\\D-Fire\\test\\labels.cache... 4306 images, 2005 backgro\u001b[0mIOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\yolo_v11n_224px3\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.002), 87 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 224 train, 224 val\n",
      "Using 1 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\yolo_v11n_224px3\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50      2.66G      2.046      3.656      1.677        653        224:  21%|██▏       | 15/70 [02:06<09:20, 10."
     ]
    }
   ],
   "source": [
    "# Load pretrained YOLO model for transfer learning\n",
    "#!pip install ultralytics\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Training options\n",
    "opts = {\n",
    "    \"data\": \"datasets\\\\data.yaml\",\n",
    "    \"batch\": 256,\n",
    "    \"val\": True,\n",
    "    \"optimizer\": \"auto\",\n",
    "    \"pretrained\": True,\n",
    "    \"freeze\": 11, # transfer learning\n",
    "    \"imgsz\": 224, # larger is better for texture recognition but slow\n",
    "    \"plots\": True,\n",
    "    \"workers\": 1, # fix for old Nvidia GPU\n",
    "}\n",
    "\n",
    "# Train model to establish baseline performance\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "model.train(**opts, epochs=50, name=\"yolo_v11n_224px\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "475c44ba-9bf9-44ed-9ee9-9a89e916d309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load best-fit model\n",
    "model = YOLO(\"runs/detect/yolo_v11n_defaults/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764c0343-d29b-4fbe-9517-39bb075f63bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL SIZE REDUCTION\n",
    "\n",
    "# Prune model\n",
    "from torch.nn.utils import prune\n",
    "\n",
    "ratio = 0.25\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if \".conv\" in name:\n",
    "        print(f\"Pruning layer {name}...\")\n",
    "        prune.l1_unstructured(module, name='weight', amount=ratio)\n",
    "        prune.remove(module, 'weight')\n",
    "print(\"Pruning done\")\n",
    "\n",
    "#TODO:\n",
    "# Loop Fusion\n",
    "# Kernel Tuning Optimizations\n",
    "# FP16 Quantization\n",
    "\n",
    "# Fine-tune after pruning0\n",
    "model.train(**opts | {\"freeze\": False, \"epochs\":5, \"name\":\"yolo_v11n_pruned\"}) #override optimizer and lr\n",
    "model.save(f'pruned_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bbeb5bb-14b8-48ef-95a1-9f032ef768bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 (no detections), 21.9ms\n",
      "1: 640x640 (no detections), 21.9ms\n",
      "2: 640x640 1 smoke, 21.9ms\n",
      "3: 640x640 1 smoke, 21.9ms\n",
      "4: 640x640 1 smoke, 21.9ms\n",
      "5: 640x640 2 smokes, 6 fires, 21.9ms\n",
      "6: 640x640 (no detections), 21.9ms\n",
      "7: 640x640 2 smokes, 4 fires, 21.9ms\n",
      "8: 640x640 4 fires, 21.9ms\n",
      "9: 640x640 (no detections), 21.9ms\n",
      "Speed: 5.5ms preprocess, 21.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "# Test on random inputs from dataset\n",
    "import random\n",
    "import os\n",
    "\n",
    "random.seed()\n",
    "num_imgs = 5\n",
    "\n",
    "img_dir = \"datasets/Combined\" + \"/test/images\"\n",
    "random_imgs = [random.choice(os.listdir(img_dir)) for x in range(num_imgs)]\n",
    "results = model.predict([os.path.join(img_dir, im) for im in random_imgs])\n",
    "\n",
    "for result in results:\n",
    "    #boxes = result.boxes\n",
    "    result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a968586-1964-4462-bdbd-b35d70ec65f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
